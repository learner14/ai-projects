{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import tokenizers\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306190bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fbc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3393eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()\n",
    "\n",
    "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader,\n",
    "          n_epochs, patience=2, factor=0.5, epoch_callback=None):\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=factor)\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        if epoch_callback is not None:\n",
    "            epoch_callback(model, epoch)\n",
    "        for index, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "            train_metric = metric.compute().item()\n",
    "            print(f\"\\rBatch {index + 1}/{len(train_loader)}\", end=\"\")\n",
    "            print(f\", loss={total_loss/(index+1):.4f}\", end=\"\")\n",
    "            print(f\", {train_metric=:.2%}\", end=\"\")\n",
    "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
    "        history[\"train_metrics\"].append(train_metric)\n",
    "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
    "        history[\"valid_metrics\"].append(val_metric)\n",
    "        scheduler.step(val_metric)\n",
    "        print(f\"\\rEpoch {epoch + 1}/{n_epochs},                      \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.2%}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.2%}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fceec307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def del_vars(variable_names=[]):\n",
    "    for name in variable_names:\n",
    "        try:\n",
    "            del globals()[name]\n",
    "        except KeyError:\n",
    "            pass  # ignore variables that have already been deleted\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a23a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41eeaf2",
   "metadata": {},
   "source": [
    "#### The Tatoeba project is a language-learning initiative started in 2006 by Trang Ho, where contributors have created a huge collection of text pairs from many languages. The Tatoeba Challenge dataset was created by researchers from the University of Helsinki to benchmark machine translation systems, using data extracted from the Tatoeba project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a24ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_original_valid_set, nmt_test_set = load_dataset(\n",
    "    path=\"ageron/tatoeba_mt_train\", name=\"eng-spa\",\n",
    "    split=[\"validation\", \"test\"])\n",
    "split = nmt_original_valid_set.train_test_split(train_size=0.8, seed=42)\n",
    "nmt_train_set, nmt_valid_set = split[\"train\"], split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d92641",
   "metadata": {},
   "source": [
    "### Each sample in the dataset is a dictionary containing an English text along with its Spanish translation. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d843ac78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': 'Tom tried to break up the fight.',\n",
       " 'target_text': 'Tom trató de disolver la pelea.',\n",
       " 'source_lang': 'eng',\n",
       " 'target_lang': 'spa'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bfe5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_eng_spa():  # a generator function to iterate over all training text\n",
    "    for pair in nmt_train_set:\n",
    "        yield pair[\"source_text\"]\n",
    "        yield pair[\"target_text\"]\n",
    "\n",
    "max_length = 256\n",
    "vocab_size = 10_000\n",
    "\n",
    "nmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "nmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\n",
    "nmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "nmt_tokenizer.enable_truncation(max_length=max_length)\n",
    "nmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "nmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\n",
    "nmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "PAD_ID = nmt_tokenizer.token_to_id(\"<pad>\")\n",
    "BOS_ID = nmt_tokenizer.token_to_id(\"<s>\")\n",
    "EOS_ID = nmt_tokenizer.token_to_id(\"</s>\")\n",
    "nmt_seq_length = 256\n",
    "\n",
    "def _encode_with_special_tokens(text, seq_length=nmt_seq_length):\n",
    "    token_ids = nmt_tokenizer.encode(text).ids\n",
    "    token_ids = [BOS_ID] + token_ids + [EOS_ID]\n",
    "    return token_ids[:seq_length]\n",
    "\n",
    "def _pad_to_length(token_ids, seq_length=nmt_seq_length, pad_id=PAD_ID):\n",
    "    if len(token_ids) < seq_length:\n",
    "        token_ids = token_ids + [pad_id] * (seq_length - len(token_ids))\n",
    "    return token_ids[:seq_length]\n",
    "\n",
    "def nmt_collate_fn(batch):\n",
    "    src_batch, tgt_in_batch, tgt_out_batch = [], [], []\n",
    "\n",
    "    for pair in batch:\n",
    "        src_ids = _pad_to_length(_encode_with_special_tokens(pair[\"source_text\"]))\n",
    "        tgt_ids = _encode_with_special_tokens(pair[\"target_text\"], seq_length=nmt_seq_length + 1)\n",
    "        tgt_ids = _pad_to_length(tgt_ids, seq_length=nmt_seq_length + 1)\n",
    "\n",
    "        tgt_in_ids = tgt_ids[:-1]\n",
    "        tgt_out_ids = tgt_ids[1:]\n",
    "\n",
    "        src_batch.append(src_ids)\n",
    "        tgt_in_batch.append(tgt_in_ids)\n",
    "        tgt_out_batch.append(tgt_out_ids)\n",
    "\n",
    "    src_token_ids = torch.tensor(src_batch, dtype=torch.long)\n",
    "    tgt_token_ids = torch.tensor(tgt_in_batch, dtype=torch.long)\n",
    "    tgt_labels = torch.tensor(tgt_out_batch, dtype=torch.long)\n",
    "\n",
    "    src_mask = src_token_ids != PAD_ID\n",
    "    tgt_mask = tgt_token_ids != PAD_ID\n",
    "\n",
    "    pair = SimpleNamespace(\n",
    "        src_token_ids=src_token_ids,\n",
    "        tgt_token_ids=tgt_token_ids,\n",
    "        src_mask=src_mask,\n",
    "        tgt_mask=tgt_mask,\n",
    "    )\n",
    "    return pair, tgt_labels\n",
    "\n",
    "batch_size = 64\n",
    "nmt_train_loader = DataLoader(nmt_train_set, batch_size=batch_size, shuffle=True, collate_fn=nmt_collate_fn)\n",
    "nmt_valid_loader = DataLoader(nmt_valid_set, batch_size=batch_size, shuffle=False, collate_fn=nmt_collate_fn)\n",
    "nmt_test_loader = DataLoader(nmt_test_set, batch_size=batch_size, shuffle=False, collate_fn=nmt_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b3d13",
   "metadata": {},
   "source": [
    "### The inputs have shape [batch size, sequence length, embedding size], but we are adding positional encodings of shape [sequence length, embedding size]. This works thanks to the broadcasting rules: the ith positional embedding is added to the ith token’s representation of each sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa66731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_length, embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.randn(max_length, embed_dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dropout(X + self.pos_embed[:X.size(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b6f8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 500, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 500\n",
    "embed_dim = 512\n",
    "pos_embedding = PositionalEmbedding(max_length, embed_dim)\n",
    "embeddings = torch.randn(256, 500, 512)\n",
    "embeddings_with_pos = pos_embedding(embeddings)\n",
    "\n",
    "embeddings_with_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "067c8235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 500, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 512\n",
    "pos_embedding = PositionalEmbedding(max_length, embed_dim)\n",
    "embeddings = torch.randn(256, 500, 512)\n",
    "embeddings_with_pos = pos_embedding(embeddings)\n",
    "embeddings_with_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "404b2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.h = num_heads\n",
    "        self.d = embed_dim // num_heads\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, X):\n",
    "        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1, 2)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        q = self.split_heads(self.q_proj(query))  # (B, h, Lq, d)\n",
    "        k = self.split_heads(self.k_proj(key))  # (B, h, Lk, d)\n",
    "        v = self.split_heads(self.v_proj(value))  # (B, h, Lv, d) with Lv=Lk\n",
    "        scores = q @ k.transpose(2, 3) / self.d**0.5  # (B, h, Lq, Lk)\n",
    "\n",
    "        # Masking support:\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, h, Lq, Lk)\n",
    "        if key_padding_mask is not None:\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, Lk)\n",
    "            scores = scores.masked_fill(mask, -torch.inf)  # (B, h, Lq, Lk)\n",
    "\n",
    "        weights = scores.softmax(dim=-1)  # (B, h, Lq, Lk)\n",
    "        Z = self.dropout(weights) @ v  # (B, h, Lq, d)\n",
    "        Z = Z.transpose(1, 2)  # (B, Lq, h, d)\n",
    "        Z = Z.reshape(Z.size(0), Z.size(1), self.h * self.d)  # (B, Lq, h × d)\n",
    "        return (self.out_proj(Z), weights)  # (B, Lq, h × d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74b3726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        attn, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                                 key_padding_mask=src_key_padding_mask)\n",
    "        Z = self.norm1(src + self.dropout(attn))\n",
    "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
    "        return self.norm2(Z + ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae725865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        attn1, _ = self.self_attn(tgt, tgt, tgt,\n",
    "                                  attn_mask=tgt_mask,\n",
    "                                  key_padding_mask=tgt_key_padding_mask)\n",
    "        Z = self.norm1(tgt + self.dropout(attn1))\n",
    "        attn2, _ = self.multihead_attn(Z, memory, memory, attn_mask=memory_mask,\n",
    "                                       key_padding_mask=memory_key_padding_mask)\n",
    "        Z = self.norm2(Z + self.dropout(attn2))\n",
    "        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
    "        return self.norm3(Z + ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21a99fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(encoder_layer)\n",
    "                                     for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        Z = src\n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z, mask, src_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            Z = self.norm(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "908ac4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(decoder_layer)\n",
    "                                     for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        Z = tgt\n",
    "        for layer in self.layers:\n",
    "            Z = layer(Z, memory, tgt_mask, memory_mask,\n",
    "                      tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            Z = self.norm(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2a7edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout)\n",
    "        norm1 = nn.LayerNorm(d_model)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers,\n",
    "                                          norm1)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout)\n",
    "        norm2 = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers,\n",
    "                                          norm2)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None):\n",
    "        memory = self.encoder(src, src_mask, src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n",
    "                              tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30011e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NmtTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n",
    "                 num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.pos_embed = PositionalEmbedding(max_length, embed_dim, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            embed_dim, num_heads, num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers, batch_first=True)\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, pair):\n",
    "        src_embeds = self.pos_embed(self.embed(pair.src_token_ids))\n",
    "        tgt_embeds = self.pos_embed(self.embed(pair.tgt_token_ids))\n",
    "        src_pad_mask = ~pair.src_mask.bool()\n",
    "        tgt_pad_mask = ~pair.tgt_mask.bool()\n",
    "        size = [pair.tgt_token_ids.size(1)] * 2\n",
    "        full_mask = torch.full(size, True, device=tgt_pad_mask.device)\n",
    "        causal_mask = torch.triu(full_mask, diagonal=1)\n",
    "        out_decoder = self.transformer(src_embeds, tgt_embeds,\n",
    "                                       src_key_padding_mask=src_pad_mask,\n",
    "                                       memory_key_padding_mask=src_pad_mask,\n",
    "                                       tgt_mask=causal_mask, #tgt_is_causal=True,\n",
    "                                       tgt_key_padding_mask=tgt_pad_mask)\n",
    "        return self.output(out_decoder).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a38e203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.full((5, 5), True), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbcd9af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Transformer.generate_square_subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "nmt_tr_model = NmtTransformer(vocab_size, nmt_seq_length, embed_dim=128, pad_id=0,\n",
    "                              num_heads=4, num_layers=2, dropout=0.1).to(device)\n",
    "if device == \"mps\":\n",
    "    # WORKAROUND: on MPS devices, we use our custom Transformer because the\n",
    "    # nn.Transformer module explodes during training, see PyTorch issue #141287\n",
    "    nmt_tr_model.transformer = Transformer(\n",
    "        d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2)\n",
    "\n",
    "n_epochs = 20\n",
    "xentropy = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad> tokens\n",
    "optimizer = torch.optim.NAdam(nmt_tr_model.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size)\n",
    "accuracy = accuracy.to(device)\n",
    "\n",
    "history = train(nmt_tr_model, optimizer, xentropy, accuracy,\n",
    "                nmt_train_loader, nmt_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccb2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-python",
   "language": "python",
   "name": "pytorch-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
